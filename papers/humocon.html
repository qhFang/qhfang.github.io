<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HuMoCon: Concept Discovery for Human Motion Understanding</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Header */
        .header {
            background: white;
            padding: 60px 0;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        .title {
            font-size: 3.5rem;
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 20px;
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.5rem;
            color: #7f8c8d;
            margin-bottom: 40px;
            font-weight: 300;
        }

        .authors {
            font-size: 1.1rem;
            margin-bottom: 15px;
            line-height: 1.8;
        }

        .author-link {
            color: #3498db;
            text-decoration: none;
        }

        .author-link:hover {
            text-decoration: underline;
        }

        .affiliations {
            font-size: 1rem;
            color: #7f8c8d;
            margin-bottom: 40px;
            line-height: 1.6;
        }

        .buttons {
            display: flex;
            justify-content: center;
            gap: 15px;
            flex-wrap: wrap;
        }

        .btn {
            display: inline-flex;
            align-items: center;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: 500;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(52, 152, 219, 0.3);
        }

        .btn:hover {
            background: #2980b9;
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(52, 152, 219, 0.4);
        }

        .btn i {
            margin-right: 8px;
        }

        /* Main content */
        .content {
            background: white;
            margin: 40px 0;
            border-radius: 10px;
            box-shadow: 0 2px 15px rgba(0,0,0,0.1);
            overflow: hidden;
        }

        .section {
            padding: 50px 60px;
            border-bottom: 1px solid #ecf0f1;
        }

        .section:last-child {
            border-bottom: none;
        }

        .section-title {
            font-size: 2.2rem;
            color: #2c3e50;
            margin-bottom: 30px;
            font-weight: 600;
            text-align: center;
        }

        .abstract-text {
            font-size: 1.1rem;
            line-height: 1.8;
            color: #34495e;
            text-align: justify;
            max-width: 900px;
            margin: 0 auto;
        }

        /* Teaser image */
        .teaser-container {
            text-align: center;
            margin: 40px 0;
        }

        .teaser-image {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 8px 25px rgba(0,0,0,0.15);
        }

        .placeholder-image {
            width: 100%;
            max-width: 800px;
            height: 400px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 1.2rem;
            font-weight: 500;
            margin: 0 auto;
        }

        /* Results Tables */
        .results-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        .results-table th, .results-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: center;
        }
        .results-table th {
            background-color: #3498db;
            color: white;
        }

        /* Citation */
        .citation-box {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 30px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            overflow-x: auto;
            margin: 30px 0;
        }

        /* Responsive design */
        @media (max-width: 768px) {
            .title {
                font-size: 2.5rem;
            }
            
            .subtitle {
                font-size: 1.2rem;
            }
            
            .section {
                padding: 30px 20px;
            }
            
            .buttons {
                flex-direction: column;
                align-items: center;
            }
            
            .btn {
                width: 200px;
                justify-content: center;
            }
        }

        /* Smooth scrolling */
        html {
            scroll-behavior: smooth;
        }

        /* Animations */
        .fade-in {
            opacity: 0;
            transform: translateY(30px);
            transition: opacity 0.6s ease, transform 0.6s ease;
        }

        .fade-in.visible {
            opacity: 1;
            transform: translateY(0);
        }
    </style>
</head>
<body>
    <div class="header">
        <div class="container">
            <h1 class="title">HuMoCon</h1>
            <p class="subtitle">Concept Discovery for Human Motion Understanding</p>
            
            <div class="authors">
                <a href="https://qhfang.github.io/" class="author-link">Qihang Fang</a><sup>1</sup>,
                <a href="https://scholar.google.com/citations?user=WbG27wQAAAAJ" class="author-link">Chengcheng Tang</a><sup>2</sup>,
                <a href="https://btekin.github.io/" class="author-link">Bugra Tekin</a><sup>2</sup>,
                <a href="https://shugaoma.github.io/" class="author-link">Shugao Ma</a><sup>2</sup>,
                <a href="https://yanchaoyang.github.io/" class="author-link">Yanchao Yang</a><sup>1</sup><br>
                Corresponding author.
            </div>

            <div class="affiliations">
                1 The University of Hong Kong, 2 Meta
            </div>

            <div class="buttons">
                <a href="https://arxiv.org/pdf/2505.20920" class="btn" target="_blank">
                    üìÑ Paper
                </a>
                <a href="#" class="btn">
                    üíª Code
                </a>
                <a href="https://huggingface.co/spaces/qihfang/SportsCoaching" class="btn">
                    ü§ó Demo
                </a>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="content">

            <!-- Abstract Section -->
            <div class="section fade-in">
                <h2 class="section-title">Abstract</h2>
                <p class="abstract-text">
                    We present HuMoCon, a novel motion-video understanding framework designed for advanced human behavior analysis. The core of our method is a human motion concept discovery framework that efficiently trains multi-modal encoders to extract semantically meaningful and generalizable features. HuMoCon addresses key challenges in motion concept discovery, including explicit cross-modal feature alignment and preserving high-frequency information via velocity reconstruction. Comprehensive experiments on standard benchmarks demonstrate that HuMoCon significantly outperforms state-of-the-art methods in human motion understanding. <!-- ÓàÄfileciteÓàÇturn1file1ÓàÅ ÓàÄfileciteÓàÇturn1file0ÓàÅ -->
                </p>
            </div>

            <!-- Overview Section -->
            <div class="section fade-in">
                <h2 class="section-title">Overview</h2>
                <div class="teaser-container">
                    <figure class="teaser-figure">
                        <img src="/images/humocon-teaser.png" alt="HuMoCon teaser" class="teaser-image" loading="lazy">
                        <figcaption>üéØ HuMoCon Framework Overview</figcaption>
                    </figure>
                </div>
                <p class="abstract-text">
                    HuMoCon introduces a novel approach to human motion understanding through automated concept discovery. Our framework identifies meaningful motion concepts and their relationships, enabling more interpretable and effective human behavior analysis.
                </p>
            </div>

            <!-- Method Section -->
            <div class="section fade-in">
                <h2 class="section-title">Method</h2>
                <div class="teaser-container">
                    <figure class="teaser-figure">
                        <img src="/images/humocon-pipeline.png" alt="HuMoCon overview" class="teaser-image" loading="lazy">
                        <figcaption>üèóÔ∏è HuMoCon Architecture</figcaption>
                    </figure>
                </div>
                <p class="abstract-text">
                    Our method consists of three main components: (1) Motion Encoder that processes raw motion sequences, (2) Concept Discovery Module that identifies semantic concepts via VQ-VAE-based discretization with masked and velocity reconstruction objectives, and (3) Concept Reasoning Module that establishes relationships between discovered concepts for comprehensive understanding. We explicitly align video and motion features during encoder pre-training and leverage LLM fine-tuning for downstream motion-video question answering tasks.
                </p>
            </div>

            <!-- Experiments Section -->
            <div class="section fade-in">
                <h2 class="section-title">Experiments</h2>
                <!-- Quantitative Results: BABEL-QA -->
                <h3>Quantitative Results</h3>
                <h4>BABEL-QA Benchmark</h4>
                <!-- Table from paper Table 1: Comparison on BABEL-QA -->
                <table class="results-table">
                    <thead>
                        <tr>
                            <th>Model</th><th>Pred type</th><th>Overall</th><th>Action</th><th>Direction</th><th>BodyPart</th><th>Before</th><th>After</th><th>Other</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>2s-AGCN-M</td><td>cls.</td><td>0.355</td><td>0.384</td><td>0.352</td><td>0.228</td><td>0.331</td><td>0.264</td><td>0.295</td></tr>
                        <tr><td>2s-AGCN-R</td><td>cls.</td><td>0.357</td><td>0.396</td><td>0.352</td><td>0.194</td><td>0.337</td><td>0.301</td><td>0.285</td></tr>
                        <tr><td>MotionCLIP-M</td><td>cls.</td><td>0.430</td><td>0.485</td><td>0.361</td><td>0.272</td><td>0.372</td><td>0.321</td><td>0.404</td></tr>
                        <tr><td>MotionCLIP-R</td><td>cls.</td><td>0.420</td><td>0.489</td><td>0.310</td><td>0.250</td><td>0.398</td><td>0.314</td><td>0.387</td></tr>
                        <tr><td>MotionLLM</td><td>gen.</td><td>0.436</td><td>0.517</td><td>0.354</td><td>0.154</td><td>0.427</td><td>0.368</td><td>0.529</td></tr>
                        <tr><td>Ours</td><td>gen.</td><td>0.711</td><td>0.809</td><td>0.697</td><td>0.623</td><td>0.707</td><td>0.635</td><td>0.797</td></tr>
                    </tbody>
                </table>
                <p class="abstract-text">
                    Our method outperforms baselines by a large margin on the BABEL-QA test set, achieving 0.711 overall accuracy compared to 0.436 by MotionLLM, with notable gains in BodyPart queries (0.623 vs. 0.154) ÓàÄfileciteÓàÇturn1file1ÓàÅ.
                </p>

                <h4>ActivityNet-QA Benchmark</h4>
                <!-- Table from paper Table 2: ActivityNet-QA results -->
                <table class="results-table">
                    <thead>
                        <tr><th>Model</th><th>Acc‚Üë</th><th>Score‚Üë</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>FrozenBiLM</td><td>24.7</td><td>-</td></tr>
                        <tr><td>VideoChat</td><td>-</td><td>2.2</td></tr>
                        <tr><td>LLaMA-Adapter</td><td>34.2</td><td>2.7</td></tr>
                        <tr><td>Video-LLaMA</td><td>12.4</td><td>1.1</td></tr>
                        <tr><td>Video-ChatGPT</td><td>35.2</td><td>2.7</td></tr>
                        <tr><td>Video-LLaVA</td><td>45.3</td><td>3.3</td></tr>
                        <tr><td>VideoChat2</td><td>49.1</td><td>3.3</td></tr>
                        <tr><td>MotionLLM</td><td>53.3</td><td>3.5</td></tr>
                        <tr><td>Ours</td><td>54.2</td><td>3.6</td></tr>
                    </tbody>
                </table>
                <p class="abstract-text">
                    On ActivityNet-QA, HuMoCon achieves 54.2% accuracy and a score of 3.6, outperforming previous methods including MotionLLM (53.3%, 3.5) ÓàÄfileciteÓàÇturn1file6ÓàÅ.
                </p>

                <!-- Ablation Study -->
                <h3>Ablation Study</h3>
                <h4>BABEL-QA Ablation</h4>
                <!-- Table from paper Table 3: Ablation on BABEL-QA -->
                <table class="results-table">
                    <thead>
                        <tr>
                            <th>Model</th><th>Pred type</th><th>Overall</th><th>Action</th><th>Direction</th><th>BodyPart</th><th>Before</th><th>After</th><th>Other</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>MotionLLM</td><td>gen.</td><td>0.436</td><td>0.517</td><td>0.354</td><td>0.154</td><td>0.427</td><td>0.368</td><td>0.529</td></tr>
                        <tr><td>Ours-w/oLrec</td><td>gen.</td><td>0.696</td><td>0.741</td><td>0.645</td><td>0.577</td><td>0.600</td><td>0.597</td><td>0.762</td></tr>
                        <tr><td>Ours-w/oLdis&Lact</td><td>gen.</td><td>0.637</td><td>0.693</td><td>0.478</td><td>0.606</td><td>0.667</td><td>0.526</td><td>0.709</td></tr>
                        <tr><td>Ours-w/oLalign</td><td>gen.</td><td>0.675</td><td>0.743</td><td>0.579</td><td>0.523</td><td>0.584</td><td>0.570</td><td>0.743</td></tr>
                        <tr><td>Ours</td><td>gen.</td><td>0.711</td><td>0.809</td><td>0.697</td><td>0.623</td><td>0.707</td><td>0.635</td><td>0.797</td></tr>
                    </tbody>
                </table>
                <p class="abstract-text">
                    Ablation confirms that velocity reconstruction and feature alignment are critical for performance; removing these modules degrades results significantly ÓàÄfileciteÓàÇturn1file6ÓàÅ.
                </p>


                <!-- Qualitative Examples -->
                <h3>Qualitative Results</h3>
                <div class="teaser-container">
                    <figure class="teaser-figure">
                        <img src="/images/motionvis_page1_200dpi.png" alt="HuMoCon overview" class="teaser-image" loading="lazy">
                        <figcaption>Example Q&A results demonstrate detailed motion understanding</figcaption>
                    </figure>
                </div>
                <p class="abstract-text">
                    Example Q&A results demonstrate detailed motion understanding, e.g., answering kinematic and contextual questions such as muscle engagement during push-ups and phase descriptions in jump sequences, showing HuMoCon‚Äôs capability to reason about motion sequences ÓàÄfileciteÓàÇturn1file6ÓàÅ.
                </p>
            </div>

            <!-- Citation Section -->
            <div class="section fade-in">
                <h2 class="section-title">Citation</h2>
                <div class="citation-box">
@inproceedings{Fang2025HuMoCon,
  title={HuMoCon: Concept Discovery for Human Motion Understanding},
  author={Qihang Fang and Chengcheng Tang and Bugra Tekin and Shugao Ma and Yanchao Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
                </div>
            </div>

            <!-- Acknowledgments Section -->
            <div class="section fade-in">
                <h2 class="section-title">Acknowledgments</h2>
                <p class="abstract-text">
                    This work is supported by the Early Career Scheme of the Research Grants Council (grant #27207224), the HKU-100 Award, a donation from the Musketeers Foundation, and an Academic Gift from Meta. Data collection, processing, and model development were conducted at The University of Hong Kong.
                </p>
            </div>
        </div>
    </div>

    <script>
        // Intersection Observer for fade-in animations
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.classList.add('visible');
                }
            });
        }, observerOptions);

        document.querySelectorAll('.fade-in').forEach(el => {
            observer.observe(el);
        });

        document.querySelectorAll('.btn').forEach(btn => {
            btn.addEventListener('click', function(e) {
                const ripple = document.createElement('span');
                const rect = this.getBoundingClientRect();
                const size = Math.max(rect.width, rect.height);
                const x = e.clientX - rect.left - size / 2;
                const y = e.clientY - rect.top - size / 2;
                ripple.style.cssText = `
                    position: absolute;
                    width: ${size}px;
                    height: ${size}px;
                    left: ${x}px;
                    top: ${y}px;
                    background: rgba(255, 255, 255, 0.3);
                    border-radius: 50%;
                    transform: scale(0);
                    animation: ripple 0.6s linear;
                    pointer-events: none;
                `;
                this.appendChild(ripple);
                setTimeout(() => { ripple.remove(); }, 600);
            });
        });

        const style = document.createElement('style');
        style.textContent = `
            @keyframes ripple { to { transform: scale(4); opacity: 0; } }
            .btn { position: relative; overflow: hidden; }
        `;
        document.head.appendChild(style);
    </script>
</body>
</html>
